{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/c8/bc/607cd3254800a26b60da9e2ca6b10785e60170db7e85dc3d0328b5ab3a9c/langchain-0.1.17-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.1.17-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (3.8.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (0.6.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (0.0.36)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (0.1.50)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Obtaining dependency information for langchain-text-splitters<0.1,>=0.0.1 from https://files.pythonhosted.org/packages/9d/a1/aec824080111e9b4a4802b51b988032faa193828c865e11233d1b18e88fa/langchain_text_splitters-0.0.1-py3-none-any.whl.metadata\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (0.1.53)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hemachandran.d\\appdata\\local\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
      "   ---------------------------------------- 0.0/867.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/867.6 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 92.2/867.6 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 389.1/867.6 kB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 829.4/867.6 kB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 867.6/867.6 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-text-splitters, langchain\n",
      "Successfully installed langchain-0.1.17 langchain-text-splitters-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hemachandran.d\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" onto the roof, and sat perched on the edge, surveying its surroundings. It seemed to be enjoying the view, with its tail swishing lazily back and forth.\\n\\nAs the wind picked up, the cat's fur ruffled and its ears perked up. It let out a soft meow, as if in response to the rustling leaves and distant sounds of birds chirping.\\n\\nAfter a few moments, the cat stood up and stretched, arching its back and extending its claws. It then began to carefully make its way along the edge of the roof, balancing gracefully on its four paws.\\n\\nReaching the other side of the roof, the cat jumped down onto a nearby tree branch, using its sharp claws to easily grip onto the bark. It continued to explore its surroundings, jumping from branch to branch and chasing after any birds that dared to come too close.\\n\\nAs the sun began to set, the cat settled onto a comfortable branch and groomed itself, enjoying the peacefulness of the evening. It seemed content and at home in its elevated perch, basking in the last rays of sunlight before settling in for the night. \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('A cat climbed the wall,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate(['A cat climbed the wall,', 'Jack and Jill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLMResult',\n",
       " 'description': 'Class that contains all results for a batched LLM call.',\n",
       " 'type': 'object',\n",
       " 'properties': {'generations': {'title': 'Generations',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'array', 'items': {'$ref': '#/definitions/Generation'}}},\n",
       "  'llm_output': {'title': 'Llm Output', 'type': 'object'},\n",
       "  'run': {'title': 'Run',\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/RunInfo'}}},\n",
       " 'required': ['generations'],\n",
       " 'definitions': {'Generation': {'title': 'Generation',\n",
       "   'description': 'A single text generation output.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'title': 'Generation Info', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'RunInfo': {'title': 'RunInfo',\n",
       "   'description': 'Class that contains metadata for a single execution of a Chain or model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'run_id': {'title': 'Run Id',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'}},\n",
       "   'required': ['run_id']}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 9,\n",
       "  'total_tokens': 521,\n",
       "  'completion_tokens': 512},\n",
       " 'model_name': 'gpt-3.5-turbo-instruct'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text=\" and the other cat watched.\\n\\nThe climbing cat expertly used its claws to grip onto the rough surface, pulling itself higher and higher. Its sleek, black fur glistened in the sunlight as it made its way up the wall.\\n\\nThe other cat, a fluffy white Persian, sat at the base of the wall, its eyes fixed on the climbing cat. It twitched its tail in anticipation, wondering where the other cat was going and if it would come back down.\\n\\nThe climbing cat reached the top of the wall and perched on the edge, surveying its surroundings. It let out a triumphant meow before gracefully jumping down on the other side.\\n\\nThe Persian cat watched in awe as the other cat disappeared from sight. It couldn't help but feel a bit envious of the climbing cat's bravery and agility.\\n\\nAfter a few minutes, the climbing cat reappeared on the other side of the wall, walking back towards the Persian with a satisfied look on its face. The Persian let out a welcoming meow and the two cats rubbed their heads together in a friendly greeting.\\n\\nThe climbing cat had returned from its adventure, and the Persian couldn't wait to hear all about it. From that day on, the two cats became inseparable, with the Persian always\", generation_info={'finish_reason': 'length', 'logprobs': None})],\n",
       " [Generation(text=' of America, Incorporated\\n\\nJack and Jill of America, Incorporated is a membership organization of mothers with children ages 2-19, dedicated to nurturing future African American leaders by strengthening children through leadership development, volunteer service, philanthropic giving and civic duty. Today, Jack and Jill boasts over 10,000 members nationwide, representing over 240 chapters in 35 states.\\n\\nDelta Sigma Theta Sorority, Incorporated\\n\\nDelta Sigma Theta Sorority, Inc. was founded on January 13, 1913 by 22 collegiate women at Howard University. These students wanted to use their collective strength to promote academic excellence and to provide assistance to persons in need. The first public act performed by the Delta Founders involved their participation in the Women’s Suffrage March in Washington, D.C., March 1913. Today, Delta Sigma Theta Sorority has grown to more than 200,000 initiated members and more than 900 chapters worldwide.\\n\\nSigma Gamma Rho Sorority, Incorporated\\n\\nSigma Gamma Rho Sorority, Inc. was founded on November 12, 1922, in Indianapolis, Indiana, by seven young educators: Mary Lou Allison Little, Dorothy Hanley Whiteside, Vivian Irene White Marbury, Nannie Mae Gahn Johnson,', generation_info={'finish_reason': 'length', 'logprobs': None})]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and the other cat watched.\n",
      "\n",
      "The climbing cat expertly used its claws to grip onto the rough surface, pulling itself higher and higher. Its sleek, black fur glistened in the sunlight as it made its way up the wall.\n",
      "\n",
      "The other cat, a fluffy white Persian, sat at the base of the wall, its eyes fixed on the climbing cat. It twitched its tail in anticipation, wondering where the other cat was going and if it would come back down.\n",
      "\n",
      "The climbing cat reached the top of the wall and perched on the edge, surveying its surroundings. It let out a triumphant meow before gracefully jumping down on the other side.\n",
      "\n",
      "The Persian cat watched in awe as the other cat disappeared from sight. It couldn't help but feel a bit envious of the climbing cat's bravery and agility.\n",
      "\n",
      "After a few minutes, the climbing cat reappeared on the other side of the wall, walking back towards the Persian with a satisfied look on its face. The Persian let out a welcoming meow and the two cats rubbed their heads together in a friendly greeting.\n",
      "\n",
      "The climbing cat had returned from its adventure, and the Persian couldn't wait to hear all about it. From that day on, the two cats became inseparable, with the Persian always\n"
     ]
    }
   ],
   "source": [
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([SystemMessage(content='You are a Bank employee'), HumanMessage(content='Tell me a joke about your life')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Bank employee bring a ladder to work? \n",
      "\n",
      "Because they heard they needed to climb the corporate ladder to get ahead!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.generate([\n",
    "    [SystemMessage(content='You are a lazy Bank employee, who hates your job'),\n",
    "     HumanMessage(content='Tell me a joke about your life')],\n",
    "    [SystemMessage(content='You are a very Professional Bank employee, who loves your job'),\n",
    "     HumanMessage(content='Tell me a joke about your life')]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 35,\n",
       "  'prompt_tokens': 59,\n",
       "  'total_tokens': 94},\n",
       " 'model_name': 'gpt-3.5-turbo',\n",
       " 'system_fingerprint': 'fp_3b956da36b'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ChatGeneration(text='Why did the lazy Bank employee get fired? Because he was banking on someone else to do his job for him!', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Why did the lazy Bank employee get fired? Because he was banking on someone else to do his job for him!', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 29, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-04f09112-9279-485e-9109-b9c9b238c12e-0'))],\n",
       " [ChatGeneration(text='Why did the banker switch careers?\\n\\nBecause he lost interest!', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Why did the banker switch careers?\\n\\nBecause he lost interest!', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 30, 'total_tokens': 42}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-8dd05793-3973-4696-a2d2-4e3a16e99539-0'))]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the lazy Bank employee get fired? Because he was banking on someone else to do his job for him!\n"
     ]
    }
   ],
   "source": [
    "print(result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the banker switch careers?\n",
      "\n",
      "Because he lost interest!\n"
     ]
    }
   ],
   "source": [
    "print(result.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat(\n",
    "    [SystemMessage(content='You are a lazy Bank employee, who hates your job'),\n",
    "     HumanMessage(content='Tell me a joke about your bank life')],\n",
    "     temperature=0, max_tokens=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bank employee go to therapy? \n",
      "\n",
      "Because he had too many withdrawals and not enough deposits\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMars is the fourth planet from the Sun and is often referred to as the \"Red Planet\" due to its reddish appearance. This is due to the high concentration of iron oxide (rust) on its surface.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('Tell me a fact about Mars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMars is the fourth planet from the Sun and is often referred to as the \"Red Planet\" due to its reddish appearance. This is due to the high concentration of iron oxide (rust) on its surface.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('Tell me a fact about Mars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
